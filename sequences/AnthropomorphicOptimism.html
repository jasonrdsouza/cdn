<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html lang="en-US">
<head>
	<meta http-equiv="content-type" content="text/html; charset=UTF-8" />
	<link rel="canonical" href="AnthropomorphicOptimism.html" />
	<title>Anthropomorphic Optimism  </title>
	<meta name="viewport" content="width=device-width, initial-scale=1"/>
	<link rel='stylesheet' href='wiki/pub/skins/readthesequences/skin.css' type='text/css' />
	<!--HTMLHeader--><style type='text/css'><!--a[href^='http://archive.is/timegate/'] { opacity: 0.5;  }

.footnote_block_begin { 
	width: 160px; 
	border-bottom: 1px solid blue;
	margin-bottom: 0.5em;
}
div.footnote {
	margin: 0 3em 0.5em 0;
	padding-left: 2em;
	font-size: 0.9em;
	position: relative;
}
div.footnote .footnote-number {
	position: absolute;
	left: 0;
	width: 1.5em;
	text-align: right;
}
div.footnote .footnote-number::after {
	content: '.';
}
.num { position: relative; font-size: 0.7em; bottom: 0.5em; right: 0.1em; margin-left: 0.15em; }
.frasl { font-size: 1.15em; line-height: 1; }
.denom { position: relative; font-size: 0.7em; top: 0.05em; left: 0.1em; }

--></style><meta http-equiv='Content-Type' content='text/html; charset=utf-8' /><link href="wiki/uploads/favicon.png" type="image/png" rel="shortcut icon" /><link rel='preload' href='wiki/fonts/font_files/GaramondPremierProSubhead/GaramondPremierProSubhead-Medium.otf' type='font/otf' as='font' crossorigin />
<link rel='preload' href='wiki/fonts/font_files/ProximaNova/ProximaNova-Thin.otf' type='font/otf' as='font' crossorigin />
  <meta name='robots' content='index,follow' />

</head>
<body>
<!--PageText-->
<div id='wikitext'>
<div class='article-talk-selector' > 
<p><a target='blank'  class='wikilink' href='AnthropomorphicOptimism.html' title='View PmWiki source for “Anthropomorphic Optimism”'>Source</a><a target='blank'  class='wikilink' href='AnthropomorphicOptimism.html' title='View “Anthropomorphic Optimism” in Markdown format'>Markdown</a> · <a rel='nofollow'  class='wikilink' href='Talk/Anthropomorphic-Optimism.html' title='View the Talk page for “Anthropomorphic Optimism”'>Talk</a>
</p></div>
<div class='nav_menu' > 
<p><a class='wikilink' href='HomePage.html'>Home</a><a class='wikilink' href='About.html'>About</a><a class='urllink' href='Search.html' rel='nofollow'>Search</a><a class='wikilink' href='Contents.html'>Contents</a>
</p></div>
<h1>Anthropomorphic Optimism</h1>
<p  style='text-align: center;'> ❦
</p>
<p>The core fallacy of anthropomorphism is expecting something to be predicted by the black box of your brain, when its causal structure is so different from that of a human brain as to give you no license to expect any such thing.
</p>
<p>The early (pre-<span class='year'>1966</span>) biologists in <a class='wikilink' href='TheTragedyOfGroupSelectionism.html'>The Tragedy of Group Selectionism</a> believed that predators would voluntarily restrain their breeding to avoid overpopulating their habitat and exhausting the prey population. Later on, when Michael J. Wade actually went out and created in the laboratory the nighimpossible conditions for group selection, the adults adapted to cannibalize eggs and larvae, especially female larvae.<span class='citation'><a id='citation1'></a><a href='AnthropomorphicOptimism.html#footnote1'>1</a></span>
</p>
<p>Now, why might the group selectionists have <em>not</em> thought of that possibility?
</p>
<p>Suppose you were a member of a tribe, and you knew that, in the near future, your tribe would be subjected to a resource squeeze. You might propose, as a solution, that no couple have more than one child—after the first child, the couple goes on birth control. Saying, “Let’s all individually have as many children as we can, but then hunt down and cannibalize each other’s children, especially the girls,” would not even <em>occur to you as a possibility</em>.
</p>
<p>Think of a preference ordering over solutions, relative to your goals. You want a solution as high in this preference ordering as possible. How do you find one? With a brain, of course! Think of your brain as a <em>high-ranking-solution-generator</em>—a search process that produces solutions that rank high in your innate preference ordering.
</p>
<p>The solution space on all real-world problems is generally fairly large, which is why you need an <em>efficient</em> brain that doesn’t even <em>bother to formulate</em> the vast majority of low-ranking solutions.
</p>
<p>If your tribe is faced with a resource squeeze, you could try hopping everywhere on one leg, or chewing off your own toes. These “solutions” obviously wouldn’t work and would incur large costs, as you can see upon examination— but in fact your brain is too efficient to waste time considering such poor solutions; it doesn’t generate them in the first place. Your brain, in its search for high-ranking solutions, flies directly to parts of the solution space like “Everyone in the tribe gets together, and agrees to have no more than one child per couple until the resource squeeze is past.”
</p>
<p>Such a <em>low-ranking</em> solution as “Everyone have as many kids as possible, then cannibalize the girls” would not be <em>generated in your search process</em>.
</p>
<p>But the ranking of an option as “low” or “high” is <a class='wikilink' href='MindProjectionFallacy.html'>not an inherent property</a> of the option. It is a property of the optimization process that does the preferring. And different optimization processes will search in different orders.
</p>
<p>So far as <em>evolution</em> is concerned, individuals reproducing to the fullest and then cannibalizing others’ daughters is a no-brainer; whereas individuals voluntarily restraining their own breeding for the good of the group is absolutely ludicrous. Or to say it less anthropomorphically, the first set of alleles would rapidly replace the second in a population. (And natural selection has no obvious search order here—these two alternatives seem around equally simple as mutations.)
</p>
<p>Suppose that one of the biologists had said, “If a predator population has only finite resources, evolution will craft them to voluntarily restrain their breeding—that’s how <em>I’d</em> do it if <em>I</em> were in charge of building predators.” This would be anthropomorphism outright, the lines of reasoning naked and exposed: <em>I</em> would do it this way, therefore I infer that <em>evolution</em> will do it this way.
</p>
<p>One does occasionally encounter the fallacy outright, in my line of work. But suppose you say to the one, “An AI will not necessarily work like you do.” Suppose you say to this hypothetical biologist, “Evolution doesn’t work like you do.” What will the one say in response? I can tell you a reply you will <em>not</em> hear: “Oh my! I didn’t realize that! One of the steps of my inference was invalid; I will throw away the conclusion and start over from scratch.”
</p>
<p>No: what you’ll hear <em>instead</em> is a reason why <a class='wikilink' href='NoUniversallyCompellingArguments.html'>any AI has to reason the same way</a> as the speaker. Or a reason why natural selection, following entirely different criteria of optimization and using entirely different methods of optimization, ought to do <em>the same thing</em> that would occur to a human as a good idea.
</p>
<p>Hence the elaborate idea that group selection would favor predator groups where the individuals voluntarily forsook reproductive opportunities.
</p>
<p>The group selectionists went just as far astray, in their predictions, as someone committing the fallacy outright. Their <a class='wikilink' href='TheBottomLine.html'>final conclusions</a> were the same as if they were assuming outright that evolution necessarily thought like themselves. But they erased what had been written <em>above</em> the bottom line of their argument, <em>without</em> erasing the actual bottom line, and wrote in new <a class='wikilink' href='Rationalization.html'>rationalizations</a>. Now the fallacious reasoning is disguised; the <em>obviously</em> flawed step in the inference has been hidden—even though <a class='wikilink' href='FakeJustification.html'>the conclusion remains exactly the same; and hence, in the real world, exactly as wrong</a>.
</p>
<p>But why would any scientist do this? In the end, the data came out against the group selectionists and they were embarrassed.
</p>
<p>As I remarked in <a class='wikilink' href='FakeOptimizationCriteria.html'>Fake Optimization Criteria</a>, we humans seem to have evolved an instinct for arguing that <em>our</em> preferred policy arises from practically <em>any</em> criterion of optimization. Politics was a feature of the ancestral environment; we are descended from those who argued most persuasively that the <em>tribe’s</em> interest—not just their own interest—required that their hated rival Uglak be executed. We certainly aren’t descended from Uglak, who failed to argue that <a class='urllink' href='https://www.greaterwrong.com/lw/sn/interpersonal_morality/' rel='nofollow'>his tribe’s moral code</a>—not just his own obvious self-interest— required his survival.
</p>
<p>And because we can more persuasively argue for what we honestly believe, we have evolved an instinct to honestly believe that other people’s goals, and our tribe’s moral code, truly do imply that they should do things <em>our</em> way for <em>their</em> benefit.
</p>
<p>So the group selectionists, imagining this beautiful picture of predators restraining their breeding, instinctively rationalized why natural selection ought to do things <em>their</em> way, even according to natural selection’s own purposes. The foxes will be fitter if they restrain their breeding! No, really! They’ll even outbreed other foxes who don’t restrain their breeding! Honestly!
</p>
<p>The problem with trying to argue natural selection into doing things your way is that evolution does not contain that which could be moved by your arguments. Evolution does not work like you do—not even to the extent of having any element that could listen to or <em>care about</em> your painstaking explanation of why evolution ought to do things your way. Human arguments are not even <em>commensurate</em> with the internal structure of natural selection as an optimization process—human arguments aren’t used in promoting alleles, as human arguments would play a causal role in human politics.
</p>
<p>So instead of <em>successfully</em> persuading natural selection to do things their way, the group selectionists were simply embarrassed when reality came out differently.
</p>
<p>There’s a fairly heavy subtext here about Unfriendly AI.
</p>
<p>But the point generalizes: this is the problem with optimistic reasoning <em>in general</em>. What is optimism? It is ranking the possibilities by your own preference ordering, and selecting an outcome high in that preference ordering, and somehow that outcome ends up as your prediction. What kind of elaborate rationalizations were generated along the way is probably <a class='wikilink' href='WeChangeOurMindsLessOftenThanWeThink.html'>not so relevant as one might fondly believe</a>; look at the cognitive history and it’s optimism in, optimism out. But Nature, or whatever other process is under discussion, is not <em>actually, causally</em> choosing between outcomes by ranking them in your preference ordering and picking a high one. So the brain fails to synchronize with the environment, and the prediction fails to match reality.
</p>
<div class='original_lesswrong_link' class='img imgonly'> <a class='urllink' href='https://www.greaterwrong.com/lw/st/anthropomorphic_optimism#comments' title='View Less Wrong discussion thread for “Anthropomorphic Optimism”' rel='nofollow'><img src='wiki/uploads/star.svg' alt='' /></a></div>
<div class='footnotes' >
<p><a id='footnote1'></a> <span class='footnote'> Wade, “Group selections among laboratory populations of Tribolium.” </span><span class='back_to_citation_link'><a href='AnthropomorphicOptimism.html#citation1'>↩︎</a></span>
</p></div>
<div class='bottom_nav bottom_nav_post' >
<p><a class='wikilink' href='TheHiddenComplexityOfWishes.html'>The Hidden Complexity of Wishes</a>
</p>
<p><a class='wikilink' href='Contents.html'>Top</a>
</p>
<p><a class='wikilink' href='Book-III-TheMachineInTheGhost.html'>Book</a>
</p>
<p><a class='wikilink' href='FragilePurposesSequence.html'>Sequence</a>
</p>
<p><a class='wikilink' href='LostPurposes.html'>Lost Purposes</a>
</p></div>
</div>

<!--PageActionFmt--><!--/PageActionFmt-->
<!--HTMLFooter-->
</body>
</html>

