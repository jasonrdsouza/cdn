<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html lang="en-US">
<head>
	<meta http-equiv="content-type" content="text/html; charset=UTF-8" />
	<link rel="canonical" href="WhyOurKindCantCooperate.html" />
	<title>Why Our Kind Can’t Cooperate  </title>
	<meta name="viewport" content="width=device-width, initial-scale=1"/>
	<link rel='stylesheet' href='wiki/pub/skins/readthesequences/skin.css' type='text/css' />
	<!--HTMLHeader--><style type='text/css'><!--a[href^='http://archive.is/timegate/'] { opacity: 0.5;  }

.footnote_block_begin { 
	width: 160px; 
	border-bottom: 1px solid blue;
	margin-bottom: 0.5em;
}
div.footnote {
	margin: 0 3em 0.5em 0;
	padding-left: 2em;
	font-size: 0.9em;
	position: relative;
}
div.footnote .footnote-number {
	position: absolute;
	left: 0;
	width: 1.5em;
	text-align: right;
}
div.footnote .footnote-number::after {
	content: '.';
}
.num { position: relative; font-size: 0.7em; bottom: 0.5em; right: 0.1em; margin-left: 0.15em; }
.frasl { font-size: 1.15em; line-height: 1; }
.denom { position: relative; font-size: 0.7em; top: 0.05em; left: 0.1em; }

--></style><meta http-equiv='Content-Type' content='text/html; charset=utf-8' /><link href="wiki/uploads/favicon.png" type="image/png" rel="shortcut icon" /><link rel='preload' href='wiki/fonts/font_files/GaramondPremierProSubhead/GaramondPremierProSubhead-Medium.otf' type='font/otf' as='font' crossorigin />
<link rel='preload' href='wiki/fonts/font_files/ProximaNova/ProximaNova-Thin.otf' type='font/otf' as='font' crossorigin />
  <meta name='robots' content='index,follow' />

</head>
<body>
<!--PageText-->
<div id='wikitext'>
<div class='article-talk-selector' > 
<p><a target='blank'  class='wikilink' href='WhyOurKindCantCooperate.html' title='View PmWiki source for “Why Our Kind Can’t Cooperate”'>Source</a><a target='blank'  class='wikilink' href='WhyOurKindCantCooperate.html' title='View “Why Our Kind Can’t Cooperate” in Markdown format'>Markdown</a> · <a rel='nofollow'  class='wikilink' href='Talk/Why-Our-Kind-Cant-Cooperate.html' title='View the Talk page for “Why Our Kind Can’t Cooperate”'>Talk</a>
</p></div>
<div class='nav_menu' > 
<p><a class='wikilink' href='HomePage.html'>Home</a><a class='wikilink' href='About.html'>About</a><a class='urllink' href='Search.html' rel='nofollow'>Search</a><a class='wikilink' href='Contents.html'>Contents</a>
</p></div>
<h1>Why Our Kind Can’t Cooperate</h1>
<p  style='text-align: center;'> ❦
</p>
<p>From when I was still forced to attend, I remember our synagogue’s annual fundraising appeal. It was a simple enough format, if I recall correctly. The rabbi and the treasurer talked about the shul’s expenses and how vital this annual fundraise was, and then the synagogue’s members called out their pledges from their seats.
</p>
<p>Straightforward, yes?
</p>
<p>Let me tell you about a different annual fundraising appeal. One that I ran, in fact, during the early years of the Machine Intelligence Research Institute. One difference was that the appeal was conducted over the Internet. And another difference was that the audience was largely drawn from the atheist / libertarian / technophile / science fiction fan / early adopter / programmer / etc. crowd. (To point in the rough direction of an empirical cluster in person-space. If you understood the phrase “empirical cluster in personspace” then you know who I’m talking about.)
</p>
<p>I crafted the fundraising appeal with care. By my nature I’m too proud to ask other people for help; but I’ve gotten over around 60% of that reluctance over the years. The nonprofit needed money and was growing too slowly, so I put some force and poetry into that year’s annual appeal. I sent it out to several mailing lists that covered most of our potential support base.
</p>
<p>And almost immediately, people started posting to the mailing lists about why they weren’t going to donate. Some of them raised basic questions about the nonprofit’s philosophy and mission. Others talked about their brilliant ideas for all the <em>other</em> sources that the nonprofit could get funding from, instead of them. (They didn’t volunteer to contact any of those sources <em>themselves</em>, they just had ideas for how <em>we</em> could do it.)
</p>
<p>Now you might say, “Well, maybe your mission and philosophy <em>did</em> have basic problems—you wouldn’t want to <em>censor</em> that discussion, would you?”
</p>
<p>Hold on to that thought.
</p>
<p>Because people <em>were</em> donating. We started getting donations right away, via Paypal. We even got congratulatory notes saying how the appeal had finally gotten them to start moving. A donation of $111.11 was accompanied by a message saying, “I decided to give a little bit more. One more hundred, one more ten, one more single, one more dime, and one more penny. All may not be for one, but this one is trying to be for all.”
</p>
<p>But none of those donors posted their agreement to the mailing list. Not one.
</p>
<p>So far as any of those donors knew, they were alone. And when they tuned in the next day, they discovered not thanks, but arguments for why they <em>shouldn’t</em> have donated. The criticisms, the justifications for not donating—<em>only those</em> were displayed proudly in the open.
</p>
<p>As though the treasurer had finished his annual appeal, and everyone <em>not</em> making a pledge had proudly stood up to call out justifications for refusing; while those making pledges whispered them quietly, so that no one could hear.
</p>
<p>I know someone with a rationalist cause who goes around plaintively asking, “How come the Raëlian flying-saucer cult can get tens of thousands of members  around 40,000] interested in complete nonsense, but we can’t even get a thousand people working on this?”
</p>
<p><a class='urllink' href='https://www.greaterwrong.com/lw/uz/protected_from_myself/' rel='nofollow'>The obvious wrong way</a> to finish this thought is to say, “Let’s do what the Raëlians do! Let’s add some nonsense to this meme!” For the benefit of those not immediately stopped by their ethical inhibitions, I will observe that there may be a hundred failed flying saucer cults for every one that becomes famous. And the Dark Side may require non-obvious skills, which <em>you</em>, yes <em>you</em>, do not have: Not everyone can be a Sith Lord. In particular, if you talk about your planned lies on the public Internet, you fail. I’m no master criminal, but even I can tell certain people are not cut out to be crooks.
</p>
<p>So it’s probably not a good idea to cultivate a sense of violated entitlement at the thought that some <em>other</em> group, who you think ought to be <em>inferior</em> to you, has more money and followers. That path leads to—pardon the expression—the Dark Side.
</p>
<p>But it probably <em>does</em> make sense to start asking ourselves some pointed questions, if supposed “<a class='wikilink' href='What-Do-I-Mean-By-Rationality.html'>rationalists</a>” can’t manage to <em>coordinate</em> as well as a flying saucer cult.
</p>
<p>How do things work on the Dark Side?
</p>
<p>The respected leader speaks, and there comes a chorus of pure agreement: if there are any who harbor inward doubts, they keep them to themselves. So all the individual members of the audience see this atmosphere of pure agreement, and they feel more confident in the ideas presented—even if they, personally, harbored inward doubts, why, everyone <em>else</em> seems to agree with it.
</p>
<p>(“<a class='wikilink' href='OnExpressingYourConcerns.html'>Pluralistic ignorance</a>” is the standard label for this.)
</p>
<p>If anyone is still unpersuaded after that, they leave the group (or in some places, are executed)—and the remainder are more in agreement, and reinforce each other with less interference.
</p>
<p>(I call that “<a class='wikilink' href='EvaporativeCoolingOfGroupBeliefs.html'>evaporative cooling of groups</a>.”)
</p>
<p>The <em>ideas</em> themselves, not just the leader, generate unbounded enthusiasm and praise. The <a class='wikilink' href='TheHaloEffect.html'>halo effect</a> is that perceptions of all positive qualities correlate—e.g. telling subjects about the benefits of a food preservative made them judge it as lower-risk, even though the quantities were logically uncorrelated. This can create a positive feedback effect that makes an idea seem better and better and better, especially if <a class='wikilink' href='UncriticalSupercriticality.html'>criticism is perceived as traitorous or sinful</a>.
</p>
<p>(Which I term the “<a class='wikilink' href='AffectiveDeathSpirals.html'>affective death spiral</a>.”)
</p>
<p>So these are all examples of strong Dark Side forces that can bind groups together.
</p>
<p>And presumably <em>we</em> would not go so far as to dirty our hands with such…
</p>
<p>Therefore, as a group, the Light Side will always be divided and weak. Technophiles, nerds, scientists, and even non-fundamentalist religions will never be capable of acting with the fanatic unity that animates radical Islam. Technological advantage can only go so far; your tools can be copied or stolen, and used against you. In the end the Light Side will always lose in any group conflict, and the future inevitably belongs to the Dark.
</p>
<p>I think that a person’s reaction to this prospect says a lot about their attitude towards “rationality.”
</p>
<p>Some “Clash of Civilizations” writers seem to accept that the Enlightenment is destined to lose out in the long run to radical Islam, and sigh, and shake their heads sadly. I suppose they’re trying to <a class='urllink' href='https://www.greaterwrong.com/lw/ym/cynical_about_cynicism/' rel='nofollow'>signal their cynical sophistication</a> or something.
</p>
<p>For myself, I always thought—call me loony—that a <em>true</em> rationalist ought to be <em>effective in the real world</em>.
</p>
<p>So I have a problem with the idea that the Dark Side, thanks to their <em>pluralistic ignorance and affective death spirals</em>, will always win because they are <em>better coordinated</em> than us.
</p>
<p>You would think, perhaps, that <em>real</em> rationalists ought to be <em>more</em> coordinated? Surely all that unreason must have its disadvantages? That mode can’t be <em>optimal</em>, can it?
</p>
<p>And if current “rationalist” groups <em>cannot</em> coordinate—if they can’t support group projects so well as a single synagogue draws donations from its members—well, I leave it to you to finish that syllogism.
</p>
<p>There’s a saying I sometimes use: “It is dangerous to be half a rationalist.”
</p>
<p>For example, I can think of ways to sabotage someone’s intelligence by <em>selectively</em> teaching them certain methods of rationality. Suppose you taught someone a long list of logical fallacies and cognitive biases, and trained them to spot those fallacies and biases in other people’s arguments. But you are careful to pick those fallacies and biases that are <em>easiest to accuse</em> others of, the most general ones that can easily be misapplied. And you do <em>not</em> <a class='wikilink' href='KnowingAboutBiasesCanHurtPeople.html'>warn them</a> to scrutinize <em>arguments they agree with</em> just as hard as they scrutinize <em>incongruent</em> arguments for flaws. So they have acquired a great repertoire of flaws of which to accuse only arguments and arguers who they don’t like. This, I suspect, is one of the primary ways that smart people end up stupid. (And note, by the way, that I have just given you another Fully General Counterargument against smart people whose arguments you don’t like.)
</p>
<p>Similarly, if you wanted to ensure that a group of “rationalists” never accomplished any task requiring more than one person, you could teach them only techniques of individual rationality, without mentioning anything about techniques of coordinated group rationality.
</p>
<p>I’ll write more later on how I think rationalists might be able to coordinate better. But here I want to focus on what you might call the <em>culture of disagreement</em>, or even the <em>culture of objections</em>, which is one of the two major forces preventing the technophile crowd from coordinating.
</p>
<p>Imagine that you’re at a conference, and the speaker gives a thirty-minute talk. Afterward, people line up at the microphones for questions. The first questioner objects to the graph used in slide 14 using a logarithmic scale; they quote Tufte on <em>The Visual Display of Quantitative Information</em>. The second questioner disputes a claim made in slide 3. The third questioner suggests an alternative hypothesis that seems to explain the same data…
</p>
<p>Perfectly normal, right? Now imagine that you’re at a conference, and the speaker gives a thirty-minute talk. People line up at the microphone.
</p>
<p>The first person says, “I agree with everything you said in your talk, and I think you’re brilliant.” Then steps aside.
</p>
<p>The second person says, “Slide 14 was beautiful, I learned a lot from it. You’re awesome.” Steps aside.
</p>
<p>The third person—
</p>
<p>Well, you’ll never know what the third person at the microphone had to say, because by this time, you’ve fled screaming out of the room, propelled by a bone-deep terror as if Cthulhu had erupted from the podium, the fear of the impossibly unnatural phenomenon that has invaded your conference.
</p>
<p>Yes, a group that can’t tolerate disagreement is not rational. But if you tolerate <em>only</em> disagreement—if you tolerate disagreement <em>but not agreement</em>—then you also are not rational. You’re only willing to hear some honest thoughts, but not others. You are a dangerous half-a-rationalist.
</p>
<p>We are as uncomfortable <em>together</em> as flying-saucer cult members are uncomfortable <em>apart</em>. That can’t be right either. <a class='createlinktext' rel='nofollow'
    href='https://www.readthesequences.com/ReversedStupidtyIsNotIntelligence'>Reversed stupidity is not intelligence.</a><a rel='nofollow' 
    class='createlink' href='https://www.readthesequences.com/ReversedStupidtyIsNotIntelligence'>?</a>
</p>
<p>Let’s say we have two groups of soldiers. In group 1, the privates are ignorant of tactics and strategy; only the sergeants know anything about tactics and only the officers know anything about strategy. In group 2, everyone at all levels knows all about tactics and strategy.
</p>
<p>Should we expect group 1 to defeat group 2, because group 1 will follow orders, while everyone in group 2 comes up with <em>better ideas</em> than whatever orders they were given?
</p>
<p>In this case I have to question how much group 2 really understands about military theory, because it is an <em>elementary</em> proposition that an uncoordinated mob gets slaughtered.
</p>
<p>Doing worse with <em>more knowledge</em> means you are doing something very wrong. You should always be able to <em>at least</em> implement the same strategy you would use if you are ignorant, and preferably do <em>better</em>. You definitely should not do <em>worse</em>. If you find yourself regretting your “rationality” <a class='wikilink' href='NewcombsProblemAndRegretOfRationality.html'>then you should reconsider what is rational</a>.
</p>
<p>On the other hand, if you are only half-a-rationalist, you can <em>easily</em> do worse with more knowledge. I recall a <a class='wikilink' href='KnowingAboutBiasesCanHurtPeople.html'>lovely experiment</a> which showed that politically opinionated students with more knowledge of the issues reacted less to incongruent evidence, because they had more ammunition with which to counter-argue only incongruent evidence.
</p>
<p>We would seem to be stuck in an awful valley of partial rationality where we end up more poorly coordinated than religious fundamentalists, able to put forth less effort than flying-saucer cultists. True, what little effort we <em>do</em> manage to put forth may be better-targeted at helping people rather than the reverse—but that is not an acceptable excuse.
</p>
<p>If I were setting forth to systematically train rationalists, there would be lessons on how to disagree and lessons on how to agree, lessons intended to make the trainee more comfortable with dissent, and lessons intended to make them more comfortable with conformity. One day everyone shows up dressed differently, another day they all show up in uniform. You’ve got to cover both sides, or you’re only half a rationalist.
</p>
<p>Can you imagine training prospective rationalists to wear a uniform and march in lockstep, and practice sessions where they agree with each other and applaud everything a speaker on a podium says? It sounds like unspeakable horror, doesn’t it, like the whole thing has admitted outright to being an evil <a class='wikilink' href='CultishCountercultishness.html'>cult</a>? But why is it <em>not</em> okay to practice that, while it <em>is</em> okay to practice <em>disagreeing</em> with everyone else in the crowd? Are you <em>never</em> going to have to agree with the majority?
</p>
<p>Our culture puts all the emphasis on heroic disagreement and <a class='wikilink' href='LonelyDissent.html'>heroic defiance</a>, and none on heroic agreement or heroic group consensus. We signal our superior intelligence and our <em>membership in the nonconformist community</em> by inventing clever objections to others’ arguments. Perhaps <em>that</em> is why the technophile / Silicon Valley crowd stays marginalized, losing battles with less nonconformist factions in larger society. No, we’re not losing because we’re so superior, we’re losing because our exclusively individualist traditions sabotage our ability to cooperate.
</p>
<p>The other major component that I think sabotages group efforts in the technophile community is <em>being ashamed of strong feelings</em>. We still have the Spock archetype of rationality stuck in our heads, <a class='wikilink' href='Feeling-Rational.html'>rationality as dispassion</a>. Or perhaps a related mistake, rationality as cynicism—trying to signal your superior world-weary sophistication by showing that you care less than others. Being careful to ostentatiously, publicly look down on those so naive as to show they care strongly about anything.
</p>
<p>Wouldn’t it make you feel uncomfortable if the speaker at the podium said that they cared so strongly about, say, <a class='urllink' href='https://www.fightaging.org/' rel='nofollow'>fighting aging</a>, that they would willingly die for the cause?
</p>
<p>But it is nowhere written in either probability theory or decision theory that a rationalist should not care. I’ve looked over those equations and, really, it’s not in there.
</p>
<p>The best informal definition I’ve ever heard of rationality is “That which can be destroyed by the truth should be.” We should aspire to <a class='wikilink' href='Feeling-Rational.html'>feel the emotions that fit the facts</a>, not aspire to feel no emotion. If an emotion can be destroyed by truth, we should relinquish it. But if a cause is worth striving for, then let us by all means feel fully its importance.
</p>
<p>Some things <em>are</em> worth dying for. Yes, really! And if we can’t get comfortable with admitting it and hearing others say it, then we’re going to have trouble <em>caring</em> enough—as well as <em>coordinating</em> enough—to put some effort into group projects. You’ve got to teach both sides of it, “That which can be destroyed by the truth should be,” and “That which the truth nourishes should thrive.”
</p>
<p>I’ve heard it <a class='urllink' href='http://www.overcomingbias.com/2009/02/against-propaganda-.html' rel='nofollow'>argued</a> that the taboo against emotional language in, say, science papers, is an important part of letting the facts fight it out without distraction. That doesn’t mean the taboo should apply everywhere. I think that there are parts of life where we should learn to <em>applaud</em> strong emotional language, eloquence, and poetry. When there’s something that needs doing, poetic appeals help get it done, and, therefore, are themselves to be applauded.
</p>
<p>We need to keep our efforts to expose <em>counterproductive</em> causes and <em>unjustified</em> appeals from stomping on tasks that genuinely need doing. You need both sides of it—the willingness to turn away from counterproductive causes, and the willingness to praise productive ones; the strength to be unswayed by ungrounded appeals, and the strength to be swayed by grounded ones.
</p>
<p>I think the synagogue at their annual appeal had it right, really. They weren’t going down row by row and putting individuals on the spot, staring at them and saying, “How much will <em>you</em> donate, Mr. Schwartz?” People simply announced their pledges—not with grand drama and pride, just simple announcements—and that encouraged others to do the same. Those who had nothing to give, stayed silent; those who had objections, chose some later or earlier time to voice them. That’s probably about the way things <em>should</em> be in a sane human community—taking into account that people often have trouble getting as motivated as they wish they were, and can be helped by social encouragement to overcome this weakness of will.
</p>
<p>But even if you disagree with that part, then let us say that both supporting and countersupporting opinions should have been publicly voiced. Supporters being faced by an apparently solid wall of objections and disagreements—even if it resulted from their own uncomfortable self-censorship—is <em>not</em> group rationality. It is the mere <em>mirror image</em> of what Dark Side groups do to keep their followers. Reversed stupidity is not intelligence.
</p>
<div class='original_lesswrong_link' class='img imgonly'> <a class='urllink' href='https://www.greaterwrong.com/lw/3h/why_our_kind_cant_cooperate#comments' title='View Less Wrong discussion thread for “Why Our Kind Can’t Cooperate”' rel='nofollow'><img src='wiki/uploads/star.svg' alt='' /></a></div>
<div class='bottom_nav bottom_nav_post' >
<p><a class='wikilink' href='ThreeLevelsOfRationalityVerification.html'>Three Levels of Rationality Verification</a>
</p>
<p><a class='wikilink' href='Contents.html'>Top</a>
</p>
<p><a class='wikilink' href='Book-VI-BecomingStronger.html'>Book</a>
</p>
<p><a class='wikilink' href='TheCraftAndTheCommunitySequence.html'>Sequence</a>
</p>
<p><a class='wikilink' href='TolerateTolerance.html'>Tolerate Tolerance</a>
</p></div>
</div>

<!--PageActionFmt--><!--/PageActionFmt-->
<!--HTMLFooter-->
</body>
</html>

