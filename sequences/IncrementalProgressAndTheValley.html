<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html lang="en-US">
<head>
	<meta http-equiv="content-type" content="text/html; charset=UTF-8" />
	<link rel="canonical" href="IncrementalProgressAndTheValley.html" />
	<title>Incremental Progress and the Valley  </title>
	<meta name="viewport" content="width=device-width, initial-scale=1"/>
	<link rel='stylesheet' href='wiki/pub/skins/readthesequences/skin.css' type='text/css' />
	<!--HTMLHeader--><style type='text/css'><!--a[href^='http://archive.is/timegate/'] { opacity: 0.5;  }

.footnote_block_begin { 
	width: 160px; 
	border-bottom: 1px solid blue;
	margin-bottom: 0.5em;
}
div.footnote {
	margin: 0 3em 0.5em 0;
	padding-left: 2em;
	font-size: 0.9em;
	position: relative;
}
div.footnote .footnote-number {
	position: absolute;
	left: 0;
	width: 1.5em;
	text-align: right;
}
div.footnote .footnote-number::after {
	content: '.';
}
.num { position: relative; font-size: 0.7em; bottom: 0.5em; right: 0.1em; margin-left: 0.15em; }
.frasl { font-size: 1.15em; line-height: 1; }
.denom { position: relative; font-size: 0.7em; top: 0.05em; left: 0.1em; }

--></style><meta http-equiv='Content-Type' content='text/html; charset=utf-8' /><link href="wiki/uploads/favicon.png" type="image/png" rel="shortcut icon" /><link rel='preload' href='wiki/fonts/font_files/GaramondPremierProSubhead/GaramondPremierProSubhead-Medium.otf' type='font/otf' as='font' crossorigin />
<link rel='preload' href='wiki/fonts/font_files/ProximaNova/ProximaNova-Thin.otf' type='font/otf' as='font' crossorigin />
  <meta name='robots' content='index,follow' />

</head>
<body>
<!--PageText-->
<div id='wikitext'>
<div class='article-talk-selector' > 
<p><a target='blank'  class='wikilink' href='IncrementalProgressAndTheValley.html' title='View PmWiki source for “Incremental Progress and the Valley”'>Source</a><a target='blank'  class='wikilink' href='IncrementalProgressAndTheValley.html' title='View “Incremental Progress and the Valley” in Markdown format'>Markdown</a> · <a rel='nofollow'  class='wikilink' href='Talk/Incremental-Progress-And-The-Valley.html' title='View the Talk page for “Incremental Progress and the Valley”'>Talk</a>
</p></div>
<div class='nav_menu' > 
<p><a class='wikilink' href='HomePage.html'>Home</a><a class='wikilink' href='About.html'>About</a><a class='urllink' href='Search.html' rel='nofollow'>Search</a><a class='wikilink' href='Contents.html'>Contents</a>
</p></div>
<h1>Incremental Progress and the Valley</h1>
<p  style='text-align: center;'> ❦
</p>
<p><a class='urllink' href='https://www.greaterwrong.com/lw/7k/incremental_progress_and_the_valley/' rel='nofollow'>Rationality is systematized winning.</a>
</p>
<p>“But,” you protest, “the reasonable person <em>doesn’t</em> always win!”
</p>
<p>What do you mean by this? Do you mean that every week or two, someone who bought a <a class='wikilink' href='LotteriesAWasteOfHope.html'>lottery ticket</a> with negative expected value wins the lottery and becomes much richer than you? That is not a <em>systematic</em> loss; it is selective reporting by the media. From a statistical standpoint, lottery winners don’t exist—you would never encounter one in your lifetime, if it weren’t for the selective reporting.
</p>
<p>Even perfectly rational agents can lose. They just can’t <em>know in advance</em> that they’ll lose. They can’t <em>expect to underperform</em> any other performable strategy, or they would simply perform it.
</p>
<p>“No,” you say, “I’m talking about how startup founders strike it rich by believing in themselves and their ideas more strongly than any reasonable person would. I’m talking about how religious people are happier—”
</p>
<p>Ah. Well, here’s the thing: An <em>incremental</em> step in the direction of rationality, if the result is still irrational in other ways, does not have to yield <em>incrementally</em> more winning.
</p>
<p>The optimality theorems that we have for probability theory and decision theory are for <em>perfect</em> probability theory and decision theory. There is no companion theorem which says that, starting from some flawed initial form, every <em>incremental</em> modification of the algorithm that takes the structure closer to the ideal must yield an <em>incremental</em> improvement in performance. This has not yet been proven, because it is not, in fact, true.
</p>
<p>“So,” you say, “what point is there then in striving to be more rational? We won’t reach the perfect ideal. So we have no guarantee that our steps forward are helping.”
</p>
<p>You have no guarantee that a step <em>backward</em> will help you win, either. <a class='wikilink' href='NoSafeDefenseNotEvenScience.html'>Guarantees don’t exist</a> in the world of flesh; but, contrary to popular misconceptions, judgment under <em>uncertainty</em> is what rationality is all about.
</p>
<p>“But we have several cases where, based on either vaguely plausible-sounding reasoning, or survey data, it looks like an incremental step forward in rationality is going to make us worse off. If it’s really all about winning—if you have <a class='wikilink' href='SomethingToProtect.html'>something to protect</a> more important than any ritual of cognition—then <em>why</em> take that step?”
</p>
<p>Ah, and <em>now</em> we come to the meat of it.
</p>
<p>I can’t necessarily answer for everyone, but…
</p>
<p>My first reason is that, on a professional basis, I deal with deeply confused problems that make huge demands on precision of thought. One small mistake can lead you astray for years, and there are worse penalties waiting in the wings. An unimproved level of performance isn’t <em>enough</em>; my choice is to try to do better, or give up and go home.
</p>
<p>“But that’s just <em>you</em>. Not all of us lead that kind of life. What if you’re just trying some ordinary human task like an Internet startup?”
</p>
<p>My second reason is that I am trying to push some aspects of my art further than I have seen done. I don’t <em>know</em> where these improvements lead. The loss of failing to take a step forward is not that <em>one step</em>. It is all the <em>other</em> steps forward you could have taken, beyond that point. Robin Hanson has a saying: The problem with slipping on the stairs is not falling the height of the first step; it is that falling one step leads to falling another step. In the same way, refusing to climb one step up forfeits not the height of that step but the height of the staircase.
</p>
<p>“But again—that’s just you. Not all of us are trying to push the art into uncharted territory.”
</p>
<p>My third reason is that once I realize I have been deceived, I can’t just shut my eyes and pretend I haven’t seen it. I have <em>already taken</em> that step forward; what use to deny it to myself? I couldn’t believe in God if I tried, any more than I could believe the sky above me was green while looking straight at it. If you <em>know</em> everything you need to know in order to know that you are better off deceiving yourself, it’s <a class='wikilink' href='DoublethinkChoosingToBeBiased.html'>much too late to deceive yourself</a>.
</p>
<p>“But that realization is <em>unusual</em>; other people have an <a class='wikilink' href='NoReallyIveDeceivedMyself.html'>easier time of doublethink</a> because <a class='wikilink' href='BeliefInSelfDeception.html'>they don’t realize it’s impossible</a>. <em>You</em> go around trying to <a class='wikilink' href='DontBelieveYoullSelfDeceive.html'>actively sponsor</a> the collapse of doublethink. <em>You</em>, from a higher vantage point, may know enough to expect that this will make them unhappier. So is this out of a sadistic desire to hurt your readers, or what?”
</p>
<p>Then I finally reply that my experience so far—even in this realm of merely human possibility—<em>does</em> seem to indicate that, once you sort yourself out a bit and you aren’t doing <em>quite</em> so many other things wrong, striving for more rationality actually <em>will</em> make you better off. The long road leads out of the valley and higher than before, even in the human lands.
</p>
<p>The more I know about some particular facet of the Art, the more I can see this is so. As I’ve previously remarked, my essays may be unreflective of what a true martial art of rationality would be like, because I have only focused on answering confusing questions—not fighting akrasia, coordinating groups, or being happy. In the field of answering confusing questions—the area where I have most intensely practiced the Art—it now seems <em>massively</em> obvious that anyone who thought they were better off “staying optimistic about solving the problem” would get stomped into the <em>ground</em>. By a <em>casual student</em>.
</p>
<p>When it comes to keeping motivated, or being happy, I can’t guarantee that someone who loses their illusions will be better off—because my knowledge of these facets of rationality is still crude. If these parts of the Art have been developed systematically, I <a class='urllink' href='https://www.greaterwrong.com/lw/kj/no_one_knows_what_science_doesnt_know/' rel='nofollow'>do not know of it</a>. But even here I have gone to some considerable pains to dispel half-rational half-mistaken ideas that could get in a beginner’s way, like the idea that <a class='wikilink' href='Feeling-Rational.html'>rationality opposes feeling</a>, or the idea that <a class='urllink' href='https://www.greaterwrong.com/lw/sm/the_meaning_of_right/' rel='nofollow'>rationality opposes value</a>, or the idea that sophisticated thinkers should be <a class='urllink' href='https://www.greaterwrong.com/lw/sc/existential_angst_factory/' rel='nofollow'>angsty</a> and <a class='urllink' href='https://www.greaterwrong.com/lw/ym/cynical_about_cynicism/' rel='nofollow'>cynical</a>.
</p>
<p>And if, as I hope, someone goes on to develop the art of fighting akrasia or achieving mental well-being as thoroughly as I have developed the art of answering impossible questions, I do fully expect that those who wrap themselves in their illusions will not <em>begin</em> to compete. Meanwhile—others may do better than I, if happiness is their dearest desire, for I myself have invested little effort here.
</p>
<p>I find it hard to believe that the <em>optimally</em> motivated individual, the <em>strongest</em> entrepreneur a human being can become, is still wrapped up in a blanket of comforting overconfidence. I think they’ve probably thrown that blanket out the window and organized their mind a little <em>differently</em>. I find it hard to believe that the happiest we can possibly live, even in the realms of human possibility, involves a tiny awareness lurking in the corner of your mind that it’s all a lie. I’d rather stake my hopes on neurofeedback or Zen meditation, though I’ve tried neither.
</p>
<p>But it cannot be denied that this is a very real issue in very real life. Consider this <a class='urllink' href='https://www.greaterwrong.com/lw/2s/3_levels_of_rationality_verification/#1xv' rel='nofollow'>pair of comments</a> from <em>Less Wrong</em>:
</p>
<blockquote>
<p>I’ll be honest—my life has taken a sharp downturn since I deconverted. My theist girlfriend, with whom I was very much in love, couldn’t deal with this change in me, and after six months of painful vacillation, she left me for a co-worker. That was another six months ago, and I have been heartbroken, miserable, unfocused, and <em>extremely</em> ineffective since.
</p>
<p>Perhaps this is an example of the <a class='urllink' href='https://www.greaterwrong.com/lw/2o/soulless_morality/1q1#comments' rel='nofollow'>valley of bad rationality</a> of which PhilGoetz spoke, but I still hold my current situation higher in my preference ranking than happiness with false beliefs.
</p></blockquote>
<p><a class='urllink' href='https://www.greaterwrong.com/lw/2s/3_levels_of_rationality_verification/1ym' rel='nofollow'>And</a>:
</p>
<blockquote>
<p>My empathies: that happened to me about 6 years ago (though thankfully without as much visible vacillation).
</p>
<p>My sister, who had some Cognitive Behaviour Therapy training, reminded me that relationships are forming and breaking all the time, and given I wasn’t unattractive and hadn’t retreated into monastic seclusion, it wasn’t rational to think I’d be alone for the rest of my life (she turned out to be right). That was helpful at the times when my feelings hadn’t completely got the better of me.
</p></blockquote>
<p>So—in practice, in real life, in sober fact—those first steps can, in fact, be painful. And then things can, in fact, get better. And there is, in fact, no <em>guarantee</em> that you’ll end up higher than before. Even if in principle the path must go further, there is no guarantee that any given person will get that far.
</p>
<p>If you don’t <em>prefer</em> truth to happiness with false beliefs…
</p>
<p>Well… <em>and</em> if you are not doing anything especially precarious or confusing… and if you are not buying lottery tickets… and if you’re already <a class='urllink' href='https://www.greaterwrong.com/lw/wq/you_only_live_twice/' rel='nofollow'>signed up for cryonics</a>, a sudden ultra-high-stakes confusing acid test of rationality that illustrates the Black Swan quality of trying to bet on ignorance <em>in</em> ignorance…
</p>
<p>Then it’s not <em>guaranteed</em> that taking all the incremental steps toward rationality that you can find will leave you better off. But the vaguely plausible-sounding arguments against losing your illusions generally <em>do</em> consider just one single step, without postulating any further steps, without suggesting any attempt to regain everything that was lost and go it one better. Even the surveys are comparing the average religious person to the average atheist, not the most advanced theologians to the most advanced rationalists.
</p>
<p>But if you don’t care about the truth—<em>and</em> you have nothing to protect—<em>and</em> you’re not attracted to the thought of pushing your art as far as it can go—<em>and</em> your current life seems to be going fine—<em>and</em> you have a sense that your mental well-being depends on illusions you’d rather not think about—
</p>
<p>Then you’re probably not reading this. But if you are, then, I guess… well… (a) sign up for cryonics, and then (b) <em>stop reading </em>Less Wrong<em> before your illusions collapse! <span class='smallcaps'>Run away!</span></em>
</p>
<div class='original_lesswrong_link' class='img imgonly'> <a class='urllink' href='https://www.greaterwrong.com/lw/7k/incremental_progress_and_the_valley#comments' title='View Less Wrong discussion thread for “Incremental Progress and the Valley”' rel='nofollow'><img src='wiki/uploads/star.svg' alt='' /></a></div>
<div class='bottom_nav bottom_nav_post' >
<p><a class='wikilink' href='CollectiveApathyAndTheInternet.html'>Collective Apathy and the Internet</a>
</p>
<p><a class='wikilink' href='Contents.html'>Top</a>
</p>
<p><a class='wikilink' href='Book-VI-BecomingStronger.html'>Book</a>
</p>
<p><a class='wikilink' href='TheCraftAndTheCommunitySequence.html'>Sequence</a>
</p>
<p><a class='wikilink' href='BayesiansVsBarbarians.html'>Bayesians vs. Barbarians</a>
</p></div>
</div>

<!--PageActionFmt--><!--/PageActionFmt-->
<!--HTMLFooter-->
</body>
</html>

